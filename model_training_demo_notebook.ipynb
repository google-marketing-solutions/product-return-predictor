{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "DzQ3wytkSd4n",
      "metadata": {
        "id": "DzQ3wytkSd4n"
      },
      "source": [
        "DISCLAIMER\n",
        "Copyright 2025 Google LLC.\n",
        "\n",
        "This solution, including any related sample code or data, is made available on an “as is,” “as available,” and “with all faults” basis, solely for illustrative purposes, and without warranty or representation of any kind. This solution is experimental, unsupported and provided solely for your convenience. Your use of it is subject to your agreements with Google, as applicable, and may constitute a beta feature as defined under those agreements. To the extent that you make any data available to Google in connection with your use of the solution, you represent and warrant that you have all necessary and appropriate rights, consents and permissions to permit Google to use and process that data. By using any portion of this solution, you acknowledge, assume and accept all risks, known and unknown, associated with its usage, including with respect to your deployment of any portion of this solution in your systems, or usage in connection with your business, if at all."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efzrTwqNSgqD",
      "metadata": {
        "id": "efzrTwqNSgqD"
      },
      "source": [
        "# Step 1: Install product_return_predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1499598-dbae-4567-a003-5becc01b1ad4",
      "metadata": {
        "id": "e1499598-dbae-4567-a003-5becc01b1ad4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "cd ~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63fd18cb-db5f-4c13-b5f9-b0610ab43ca7",
      "metadata": {
        "id": "63fd18cb-db5f-4c13-b5f9-b0610ab43ca7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "cd test/product_return_predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61c4c69-33f4-4b22-a315-8f5f8e4cf58f",
      "metadata": {
        "id": "d61c4c69-33f4-4b22-a315-8f5f8e4cf58f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install product_return_predictor\n",
        "# Install immutabledict for using product_return_predictor package\n",
        "!pip install immutabledict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f79437-75cf-43b0-8eca-b58a7b94d060",
      "metadata": {
        "id": "38f79437-75cf-43b0-8eca-b58a7b94d060"
      },
      "source": [
        "# Step 2: Import General Python Packages and Python Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d662b9f8-0027-4fd1-8c16-2797b37b56b5",
      "metadata": {
        "id": "d662b9f8-0027-4fd1-8c16-2797b37b56b5"
      },
      "source": [
        "## Import General Python Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc24a3e-6709-45e0-8da2-b7d9148c97f2",
      "metadata": {
        "id": "ecc24a3e-6709-45e0-8da2-b7d9148c97f2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.cloud import storage\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO, force=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bd06f49-1cbd-408d-810b-ac24ec6c3d7b",
      "metadata": {
        "id": "8bd06f49-1cbd-408d-810b-ac24ec6c3d7b"
      },
      "source": [
        "## Import Python Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdef60c6-732b-44d9-9431-6afa87dd0699",
      "metadata": {
        "id": "cdef60c6-732b-44d9-9431-6afa87dd0699",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import product_return_predictor.constant as constant\n",
        "import product_return_predictor.product_return_predictor as product_return_predictor\n",
        "import product_return_predictor.utils as utils\n",
        "import product_return_predictor.model as model\n",
        "import product_return_predictor.data_cleaning_feature_selection as data_cleaning_feature_selection\n",
        "import product_return_predictor.model_prediction_evaluation as model_prediction_evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ab770a-d777-498c-be1e-54a7f2694f4e",
      "metadata": {
        "id": "73ab770a-d777-498c-be1e-54a7f2694f4e"
      },
      "source": [
        "# Step 3: Define Solution Parameters\n",
        "\n",
        "There are 5 types of solution parameters/attributes that need to be defined:\n",
        "\n",
        "- **GCP Product Return Project Parameters**: These parameters define the core GCP resources used by the solution.\n",
        "    - `project_id`: Your Google Cloud Project ID where the product return prediction solution will be deployed and run. All BigQuery datasets, models, and other resources created by this solution will reside within this project.\n",
        "    - `dataset_id`: The BigQuery dataset ID within your project_id where intermediate and final tables (e.g., ML-ready data, predictions) will be stored.\n",
        "    - `gcp_bq_client`: An authenticated BigQuery client object. This client is used to interact with BigQuery for querying data, creating tables, and managing datasets. This is typically initialized as bigquery.Client() which will use your default GCP credentials or those configured in your environment. You generally don't need to modify this unless you have a specific authentication setup.\n",
        "    - `gcp_storage`: An authenticated Google Cloud Storage client object. This client is used for interacting with Cloud Storage buckets, for example, to store temporary files or model artifacts. Similar to gcp_bq_client, this is usually initialized as storage.Client() and uses your configured GCP credentials.\n",
        "    - `gcp_bucket_name`: The name of a Google Cloud Storage bucket within your project_id. This bucket will be used to store temporary files, model artifacts, or other data during the pipeline execution. Provide the name of an existing or new Cloud Storage bucket. Ensure the service account running the solution has appropriate permissions (read/write) to this bucket.\n",
        "    - `location`: The geographic location for your GCP resources (e.g., BigQuery datasets, Cloud Storage buckets). It's best practice to keep all resources in the same location for performance and cost efficiency. Example Value: 'us' (United States multi-region) Specify a valid GCP region or multi-region, such as 'us', 'europe-west1', etc.\n",
        "\n",
        "    \n",
        "- **Feature Engineering Parameters**: These parameters control how your data is prepared for machine learning, including column definitions and data filtering.\n",
        "    - `use_ga4_data_for_feature_engineering`: A boolean flag indicating whether the solution should use GA4 raw data as the source for feature engineering. Example Value: True or False. Set to True if you're leveraging GA4 data. If set to False, you must provide a preprocessed table via `ml_training_table_name` and specify the corresponding column names for transaction, refund, and ID fields. When you set this false, you would need to preprocess your own data source and do all the feature engineering to prepare your data for ML model in provided `ml_training_table_name` under your GCP project and dataset used for the solution (see `project_id`, `dataset_id` above).\n",
        "    - `transaction_date_col`: The name of the column in your dataset that represents the transaction date. This is a required parameter if `use_ga4_data_for_feature_engineering` is False. If `use_ga4_data_for_feature_engineering` is True, this is automatically set. Example Value: 'transaction_date'. If not using GA4 data, ensure this matches the column name in your input table.\n",
        "    - `transaction_id_col`: The name of the column in your dataset that uniquely identifies each transaction. This is a required parameter if `use_ga4_data_for_feature_engineering` is False. If use_ga4_data_for_feature_engineering is True, this is automatically set. Example Value: 'transaction_id'. If not using GA4 data, ensure this matches the column name in your input table.\n",
        "    - `refund_value_col`: The name of the column representing the monetary value of the refund (the target variable for regression). This is a required parameter if `use_ga4_data_for_feature_engineering` is False. If `use_ga4_data_for_feature_engineering` is True, this is automatically set. Example Value: 'refund_value' If not using GA4 data, ensure this matches the column name in your input table.\n",
        "    - `refund_flag_col`: The name of the column indicating whether a transaction was refunded (a binary flag, e.g., 0 for no refund, 1 for refund; the target variable for classification). This is a required parameter if `use_ga4_data_for_feature_engineering` is False. If `use_ga4_data_for_feature_engineering` is True, this is automatically set. Example Value: 'refund_flag'. If not using GA4 data, ensure this matches the column name in your input table.\n",
        "    - `refund_proportion_col`: The name of the column representing the proportion of the original transaction amount that was refunded (another potential target variable for regression). This is a required parameter if `use_ga4_data_for_feature_engineering` is False. If `use_ga4_data_for_feature_engineering` is True, this is automatically set. Example Value: 'refund_proportion'. If not using GA4 data, ensure this matches the column name in your input table.\n",
        "    - `return_policy_window_in_days`: The number of days within which a product can be returned according to your business's return policy. This influences how refunds are identified and considered for prediction. Example Value: 30 - this means after 30 days of transaction date, customers are no longer allowed to return the products they bought. We used the return policy window to remove transactions that have not passed the return deadline for model training to avoid noise. How to Define: Set this to match your actual return policy. When `use_ga4_data_for_feature_engineering` is False, this parameter is not needed.\n",
        "    - `recency_of_data_in_days`: The number of days to look back in your historical data for model training. This parameter helps define the training data window. Example Value: 365 (approximately 1 year1) - this means we are considering using past 1 year of historical transaction and return data for model training. Choose a duration that provides sufficient historical data for training relevant models. Note that consumer behaviors and your products may have evolved over time therefore when deciding on the number please also keep the recency and relevancy of the data in mind. When `use_ga4_data_for_feature_engineering` is False, this parameter is not needed.\n",
        "\n",
        "- **Google Analytics 4 (GA4) Raw Datasets Parameters**: These parameters are crucial if you plan to use GA4 data for feature engineering.\n",
        "    - `ga4_project_id:` The Google Cloud Project ID where your GA4 raw dataset resides. This is typically a public dataset or a project you own containing your GA4 export. Example Value: 'bigquery-public-data'. if `use_ga4_data_for_feature_engineering` is False, then you can leave this as None.\n",
        "    - `ga4_dataset_id`: The BigQuery dataset ID within your ga4_project_id that contains your raw GA4 event data. Example Value: 'my_ga4_dataset_id'. if `use_ga4_data_for_feature_engineering` is False, then you can leave this as None.\n",
        "\n",
        "- **Modeling Parameters**: These parameters control the type of machine learning models used and the overall modeling approach.\n",
        "    - `regression_model_type`: Specifies the type of regression model to be used for predicting the refund_value or refund_proportion. Example Value: constant.LinearBigQueryMLModelType.LINEAR_REGRESSION. You can choose from available regression model types within the constant.LinearBigQueryMLModelType enum (e.g., LINEAR_REGRESSION).\n",
        "    - `binary_classifier_model_type`: Specifies the type of binary classification model to be used for predicting the refund_flag. Example Value: constant.LinearBigQueryMLModelType.LOGISTIC_REGRESSION. You can choose from available classification model types within the constant.LinearBigQueryMLModelType enum (e.g., LOGISTIC_REGRESSION).\n",
        "    - `is_two_step_model`: A boolean flag indicating whether to use a two-step modeling approach. In a two-step model, a binary classifier first predicts if a refund will occur, and if so, a regression model then predicts the refund value. Example Value: True. How to Define: Set to True for a two-step approach or False for a single-step model (either regression or classification directly).\n",
        "    \n",
        "\n",
        "- **Optional Parameters**: These parameters offer further customization and are often required under specific conditions, as noted in their descriptions.\n",
        "    - `ml_training_table_name`: The name of the BigQuery table containing your preprocessed, ML-ready data for model training. This parameter is required when `use_ga4_data_for_feature_engineering` is False. If you are providing your own preprocessed data, specify the BigQuery table name here and make sure the table is under your GCP project and dataset (provided based on project_id, dataset_id) for your model.\n",
        "        - **[Important Note]**: ml_training_table_name needs to be preprocessed properly with columns that represent refund value, refund proportion and refund flag.\n",
        "    - `invalid_value_threshold_for_row_removal`: The threshold (as a proportion) for removing rows during data cleaning. If a row has a proportion of invalid (e.g., null) values exceeding this threshold, the entire row will be removed. Default Value: 0.5 (50%) Adjust this value based on your data quality and tolerance for missing data.\n",
        "    - `invalid_value_threshold_for_column_removal`: The threshold (as a proportion) for removing columns during data cleaning. If a column has a proportion of invalid values exceeding this threshold, the entire column will be removed. Default Value: 0.95 (95%): Adjust this value based on your data quality. Columns with very high proportions of missing values might not be useful for modeling.\n",
        "    - `min_correlation_threshold_with_numeric_labels_for_feature_reduction`: The minimum correlation threshold used for feature reduction. Features with a correlation below this threshold with the numeric target labels (e.g., refund_value, refund_proportion) might be removed to simplify the model and prevent overfitting. Default Value: 0.1. Adjust this value to control the aggressiveness of feature reduction based on correlation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4d29a5-e15c-4e2b-adaa-6313bf3fad6b",
      "metadata": {
        "id": "0b4d29a5-e15c-4e2b-adaa-6313bf3fad6b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# GCP product return project Parameters:\n",
        "project_id = 'your-gcp-project-id'\n",
        "dataset_id = 'your_dataset_id'\n",
        "gcp_bq_client = bigquery.Client()\n",
        "gcp_storage = storage.Client()\n",
        "gcp_bucket_name='your_gcp_bucket_name'\n",
        "location='your_gcp_location_name'\n",
        "\n",
        "# Feature Enginnering Parameters:\n",
        "use_ga4_data_for_feature_engineering = True\n",
        "transaction_date_col = 'transaction_date'\n",
        "transaction_id_col='transaction_id'\n",
        "refund_value_col='refund_value'\n",
        "refund_flag_col = 'refund_flag'\n",
        "refund_proportion_col = 'refund_proportion'\n",
        "return_policy_window_in_days=30\n",
        "recency_of_data_in_days=1800\n",
        "\n",
        "# GA4 raw datasets Parameters:\n",
        "ga4_project_id = 'your-gcp-project-id-for-ga4-data'\n",
        "ga4_dataset_id = 'your_dataset_id_for_ga4_data'\n",
        "\n",
        "# Modeling Parameters:\n",
        "regression_model_type = constant.LinearBigQueryMLModelType.LINEAR_REGRESSION\n",
        "binary_classifier_model_type = constant.LinearBigQueryMLModelType.LOGISTIC_REGRESSION\n",
        "is_two_step_model=True\n",
        "\n",
        "# Optional Parameters\n",
        "ml_training_table_name = None\n",
        "\n",
        "# Validate input parameters\n",
        "if use_ga4_data_for_feature_engineering and not ga4_project_id:\n",
        "    raise ValueError('ga4_project_id should be provided when use_ga4_data_for_feature_engineering is set to True.')\n",
        "if use_ga4_data_for_feature_engineering and not ga4_dataset_id:\n",
        "    raise ValueError('ga4_dataset_id should be provided when use_ga4_data_for_feature_engineering is set to True.')\n",
        "if not use_ga4_data_for_feature_engineering and not ml_training_table_name:\n",
        "    raise ValueError('ml_training_table_name should not be None when use_ga4_data_for_feature_engineering is set to False.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91bbabe9-1a0e-41c8-957c-35fcb8b17a5a",
      "metadata": {
        "id": "91bbabe9-1a0e-41c8-957c-35fcb8b17a5a"
      },
      "source": [
        "# Step 4: Create a `ProductReturnPredictor` instance called `product_return` with all the required paramters\n",
        "\n",
        "Use the predefined parameters above to create a `product_retun` instance.\n",
        "For more details on the `ProductReturnPredictor` class, please refer to `product_return_predictor.py` module under `product_return_predictor/product_return_predictor` directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16dd4374-4029-4c46-aac6-d22b07c6908e",
      "metadata": {
        "id": "16dd4374-4029-4c46-aac6-d22b07c6908e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "product_return = product_return_predictor.ProductReturnPredictor(project_id=project_id,\n",
        "                                                                 dataset_id = dataset_id,\n",
        "                                                                 gcp_bq_client=gcp_bq_client,\n",
        "                                                                 gcp_storage=gcp_storage,\n",
        "                                                                 gcp_bucket_name=gcp_bucket_name,\n",
        "                                                                 location=location,\n",
        "                                                                 ga4_project_id=ga4_project_id,\n",
        "                                                                 ga4_dataset_id=ga4_dataset_id,\n",
        "                                                                 use_ga4_data_for_feature_engineering=use_ga4_data_for_feature_engineering,\n",
        "                                                                 transaction_date_col=transaction_date_col,\n",
        "                                                                 transaction_id_col=transaction_id_col,\n",
        "                                                                 refund_value_col=refund_value_col,\n",
        "                                                                 refund_flag_col=refund_flag_col,\n",
        "                                                                 refund_proportion_col=refund_proportion_col,\n",
        "                                                                 regression_model_type=regression_model_type,\n",
        "                                                                 binary_classifier_model_type=binary_classifier_model_type,\n",
        "                                                                 ml_training_table_name=ml_training_table_name,\n",
        "                                                                 is_two_step_model=is_two_step_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63c20cdc-709c-48fe-a068-405e33d160a7",
      "metadata": {
        "id": "63c20cdc-709c-48fe-a068-405e33d160a7"
      },
      "source": [
        "# Step 5: Data Processing and Feature Engineering (`data_processing_feature_engineering`)\n",
        "This step is where your raw data gets transformed into a format suitable for machine learning. It involves several critical sub-steps, including data cleaning, feature creation (if using GA4 data), and preparing the data for model training or prediction.\n",
        "\n",
        "The `data_processing_feature_engineering` method handles the heavy lifting of preparing your data. It intelligently adapts its process based on whether you're using Google Analytics 4 (GA4) data as your source or providing your own preprocessed data.\n",
        "\n",
        "**Here's how to call this method:**\n",
        "\n",
        "```\n",
        "product_return.data_processing_feature_engineering(\n",
        "      data_pipeline_type=constant.DataPipelineType.TRAINING,\n",
        "      recency_of_transaction_for_prediction_in_days=recency_of_transaction_for_prediction_in_days,\n",
        "      return_policy_window_in_days=return_policy_window_in_days,\n",
        "      recency_of_data_in_days=recency_of_data_in_days\n",
        ")\n",
        "```\n",
        "\n",
        "**What's Happening During This Step?**\n",
        "\n",
        "The `data_processing_feature_engineering` method orchestrates the following:\n",
        "\n",
        "- **Determining Data Source**: It checks the `use_ga4_data_for_feature_engineering parameter` (defined in the previous step). This flag dictates whether the pipeline will query GA4 raw data or use a pre-existing table you've provided.\n",
        "- **Data Ingestion and Feature Creation (If using GA4 Data)**: If `use_ga4_data_for_feature_engineering` is True, the pipeline connects to your specified GA4 BigQuery project and dataset. It executes a series of BigQuery SQL queries (defined in constant.GA4_DATA_PIPELINE_QUERY_TEMPLATES) to extract relevant transaction data and engineer features directly within BigQuery. This includes calculating metrics like `refund_value`, `refund_flag`, and `refund_proportion`. It also segregates the data into two main categories:` _ml_ready_data_for_existing_customers` and `_ml_ready_data_for_first_time_purchase`, which are then further processed.\n",
        "\n",
        "- **Data Cleaning and Preprocessing**: The extracted data is then loaded into Pandas DataFrames. A comprehensive data cleaning process is applied, which includes:\n",
        "    - **Type Conversion**: Ensuring columns have the correct data types (e.g., string, numeric, date).\n",
        "    - **Missing Value Imputation**: Filling missing string values with `'unknown'` and numeric values with 0.\n",
        "    - **Invalid Data Removal (for Training)**: If you are running the training pipeline, rows and columns with a high proportion of invalid or zero values are identified and potentially removed based on thresholds (invalid_value_threshold_for_row_removal and invalid_value_threshold_for_column_removal). This step is skipped during the prediction pipeline to avoid altering the data structure unexpectedly.\n",
        "    \n",
        "- **Feature Selection**: A feature selection pipeline is applied to identify and retain the most relevant features for modeling based on their correlation with target variables. This pipeline is trained during the training phase and then saved to Google Cloud Storage for consistent use during prediction.\n",
        "\n",
        "- **Data Transformation (Scaling and Resampling)**: Features are scaled (e.g., using `MinMaxScaler`) to normalize their ranges, which can improve model performance. If there's a significant imbalance in your target variable (e.g., very few refunds), data resampling techniques may be applied to balance the classes. This data processing pipeline is also trained and saved for reusability.\n",
        "\n",
        "- **Train/Test Split**: For the training pipeline, the data is split into training and testing sets based on your specified `train_test_split_test_size_proportion` and `transaction_date_col` (for chronological splitting). This ensures your model is evaluated on unseen data.\n",
        "\n",
        "- The processed, ML-ready data for both existing and first-time customers is then saved back to BigQuery in your specified dataset_id.\n",
        "\n",
        "\n",
        "- **Handling User-Provided Preprocessed Data (If not using GA4 Data)**: If `use_ga4_data_for_feature_engineering` is False, the solution assumes you've already performed the initial data preparation and provides an ML-ready table. It uses a BigQuery SQL query to read your `ml_training_table_name` (or `ml_prediction_table_name`) and prepares it by creating a train_test split column based on the `transaction_id_col` and `train_test_split_test_size_proportion`. This data is then saved as ML-ready tables in BigQuery.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f00c153-8a62-4ba2-9a2a-f7d8bb9881e6",
      "metadata": {
        "id": "9f00c153-8a62-4ba2-9a2a-f7d8bb9881e6"
      },
      "source": [
        "**What You Need to Provide?**\n",
        "\n",
        "For this step, you'll explicitly provide values for the following parameters when calling the function:\n",
        "\n",
        "- `data_pipeline_type`: This crucial parameter tells the pipeline whether it's preparing data for model training or generating predictions.\n",
        "    - Accepted Values:\n",
        "        - `constant.DataPipelineType.TRAINING`: Select this when you are training a new model. The pipeline will perform data cleaning, feature engineering, and split data into training and testing sets. It will also train and save the data preprocessing and feature selection pipelines to Google Cloud Storage.\n",
        "        - `constant.DataPipelineType.PREDICTION`: Choose this when you want to generate predictions using a pre-trained model. The pipeline will load the saved preprocessing and feature selection pipelines from Cloud Storage and apply them to your new data, without splitting into train/test sets.\n",
        "    - How to Provide: Directly use constant.DataPipelineType.TRAINING or constant.DataPipelineType.PREDICTION.\n",
        "\n",
        "- `return_policy_window_in_days`: An integer indicating your product return policy window in days. This is used to define the timeframe within which a return is considered valid for labeling purposes in the training data. An integer value (e.g., 30).\n",
        "    - Note: This should be provided during Step 3.\n",
        "\n",
        "- `recency_of_data_in_days`: An integer specifying the number of historical days of data to consider for model training. This helps define the scope of your training dataset. How to Provide: An integer value (e.g., 1800 for approximately 5 years).\n",
        "    - Note: This should be provided during Step 3.\n",
        "\n",
        "**Important Considerations:**\n",
        "\n",
        "- **Pre-defined Parameters**: This step heavily relies on the parameters you've set up in the initial configuration (e.g., ga4_project_id, project_id, dataset_id, gcp_bucket_name, `use_ga4_data_for_feature_engineering`, `transaction_date_col`, etc.). Ensure those are correctly defined before running this step.\n",
        "\n",
        "- **Data Availability**: Make sure your GA4 data (if `use_ga4_data_for_feature_engineering` is True) or your ml_training_table_name/ml_prediction_table_name (if `use_ga4_data_for_feature_engineering` is False) are accessible in BigQuery.\n",
        "\n",
        "- **If you have done feature engineering yourself without using the ga4 data export directly**:\n",
        "    - You will still need to run the following code to prep your dataset for modeling However, all the data cleaning, validation, feature engineering, data scaling steps will be skipped.\n",
        "    - Also, make use you turn **`use_ga4_data_for_feature_engineering`** to False, and make sure set the values for the following parameters when creating **ProductReturnPredictor** instance:\n",
        "        - `ml_training_table_name`\n",
        "        - `ml_prediction_table_name`\n",
        "        - `transaction_date_col`\n",
        "        - `transaction_id_col`\n",
        "        - `refund_value_col`\n",
        "        - `refund_flag_col`\n",
        "        - `refund_proportion_col`\n",
        "        \n",
        "\n",
        "\n",
        "- **If you decide to rely on the solution to do feature engineering for you**, then make sure to turn **`use_ga4_data_for_feature_engineering`** to True. In this case there's no need to specify the parameters listed above.\n",
        "    \n",
        "\n",
        "By understanding this step, you'll have a clear picture of how your raw data evolves into the clean, transformed, and ML-ready format necessary for building powerful prediction models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7ee1256-852e-4b26-a7b6-e3704fd54909",
      "metadata": {
        "id": "a7ee1256-852e-4b26-a7b6-e3704fd54909",
        "tags": []
      },
      "outputs": [],
      "source": [
        "product_return.data_processing_feature_engineering(\n",
        "    data_pipeline_type=constant.DataPipelineType.TRAINING,\n",
        "    return_policy_window_in_days=return_policy_window_in_days,\n",
        "    recency_of_data_in_days=recency_of_data_in_days\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "711b6c87-38ec-4c5d-88bb-40120a2b7d18",
      "metadata": {
        "id": "711b6c87-38ec-4c5d-88bb-40120a2b7d18"
      },
      "source": [
        "# Step 6: Run Modeling Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11f3a73f-1a53-4b43-a4f4-8f2ee8985d08",
      "metadata": {
        "id": "11f3a73f-1a53-4b43-a4f4-8f2ee8985d08"
      },
      "source": [
        "## Model Training, Evaluation, and Prediction (model_training_pipeline_evaluation_and_prediction)\n",
        "\n",
        "This is a central step in the Product Return Predictor solution. This method handles the entire machine learning lifecycle, from training your models on the prepared data to evaluating their performance and generating initial predictions. It also provides valuable insights into what drives these predictions through feature importance analysis.\n",
        "\n",
        "Here's how you'd typically call this method in your Colab notebook:\n",
        "```\n",
        "regression_model_type = constant.LinearBigQueryMLModelType.LINEAR_REGRESSION\n",
        "binary_classifier_model_type = constant.LinearBigQueryMLModelType.LOGISTIC_REGRESSION\n",
        "first_time_purchase = True\n",
        "\n",
        "performance_metrics_dfs, model_prediction_df, predictions_actuals_distribution, feature_importance_dfs = product_return.model_training_pipeline_evaluation_and_prediction(\n",
        "    is_two_step_model=True,\n",
        "    regression_model_type=regression_model_type,\n",
        "    binary_classifier_model_type=binary_classifier_model_type,\n",
        "    first_time_purchase=first_time_purchase,\n",
        "    num_tiers_to_create_avg_prediction=10,\n",
        "    probability_threshold_for_prediction=0.5,\n",
        "    probability_threshold_for_model_evaluation=0.5,\n",
        "    bqml_template_files_dir=constant.BQML_QUERY_TEMPLATE_FILES\n",
        ")\n",
        "```\n",
        "\n",
        "**What's Happening During This Step?**\n",
        "The `model_training_pipeline_evaluation_and_prediction` method orchestrates a sequence of critical operations:\n",
        "- **Determine Input Data Table**: The method first identifies the BigQuery table containing your ML-ready data.\n",
        "    - If `use_ga4_data_for_feature_engineering` (a parameter set previously) is True, it uses the data prepared by the data_processing_feature_engineering step, specifically for \"first-time purchases\" or \"existing customers\" based on the first_time_purchase flag you provide.\n",
        "    - If `use_ga4_data_for_feature_engineering` is False, it utilizes the ml_training_table_name you previously defined.\n",
        "\n",
        "- **Model Training (model.bigquery_ml_model_training)**: The core of this step involves training one or more BigQuery ML models.\n",
        "    - Single-Step Model: If is_two_step_model is False, it directly trains a single regression model (e.g., Linear Regression) to predict the refund_value or refund_proportion.\n",
        "    - Two-Step Model: If is_two_step_model is True, it trains two separate models:\n",
        "        - A binary classification model (e.g., Logistic Regression) to predict the refund_flag (whether a refund will occur).\n",
        "        - A regression model (e.g., Linear Regression) to predict the refund_value or refund_proportion for transactions identified as likely to be refunded by the classification model.\n",
        "    These models are trained directly within BigQuery, leveraging its powerful ML capabilities.\n",
        "\n",
        "- **Model Performance Evaluation (model_prediction_evaluation.model_performance_metrics)**: After training, the solution retrieves various performance metrics for the trained models from BigQuery.\n",
        "    - For regression models, metrics like Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and R2 are typically evaluated.\n",
        "    - For classification models, metrics such as accuracy, precision, recall, F1-score, and AUC (Area Under the Receiver Operating Characteristic Curve) are commonly assessed.\n",
        "     - If a two-step model is used, it also evaluates the combined performance of both models.\n",
        "     - These metrics are returned as Pandas DataFrames, providing a detailed understanding of how well your model performs.\n",
        "\n",
        "- **Model Prediction Generation (model_prediction_evaluation.model_prediction)**: The trained models are then used to generate predictions on the training data itself. This allows for a direct comparison between the model's predictions and the actual historical outcomes.\n",
        "\n",
        "    - **Prediction and Actuals Distribution Visualization (`model_prediction_evaluation.plot_predictions_actuals_distribution`)**: The method generates a plot showing the distribution of your model's predictions versus the actual observed values. This helps you visually inspect if the model's predictions align with the real-world data distribution.\n",
        "    - **Tier-Level Average Prediction Comparison (`model_prediction_evaluation.compare_and_plot_tier_level_avg_prediction`)**: To further assess the model's performance and understand its behavior across different prediction ranges, this step divides predictions into \"tiers\" (e.g., deciles). It then compares the average predicted refund value within each tier to the actual average refund value for that tier, providing a valuable sanity check and insights into model calibration.\n",
        "        - **Note:** We want to make sure the model differentiates between the orders with high and low predicted refund values so that we can prioritize our money and resources on high customers with high net value when it comes to activation on Google Ads. Therefore, another good way to see how the model performs is to create a chart to compared the average predicted value and actual value broken down in deciles based on the predicted refund value. If the decile level average predicted \u0026 actual values are closely aligned and there’s a big/significant difference across deciles on the predicted value, that means the model is doing a decent job overall.\n",
        "\n",
        "- **Feature Importance Analysis** (`model_prediction_evaluation.training_feature_importance`): Understanding why a model makes certain predictions is crucial. This step retrieves and visualizes the feature importance for your trained model(s). Feature importance indicates which input variables had the most significant impact on the model's predictions. This can help you understand the key drivers of product returns and validate business hypotheses.\n",
        "\n",
        "**Output:**\n",
        "The method returns several objects:\n",
        "- `performance_metrics_dfs`: A dictionary containing DataFrames of model performance metrics for each trained model.\n",
        "- `model_prediction_df`: A DataFrame with the model's predictions on the training data.\n",
        "- `predictions_actuals_distribution`: A dictionary containing descriptive statistics of the prediction and actual distributions.\n",
        "- `feature_importance_dfs`: A dictionary containing DataFrames of feature importance for each trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8310cbaa-8c23-4888-8b97-66ebae846ee3",
      "metadata": {
        "id": "8310cbaa-8c23-4888-8b97-66ebae846ee3"
      },
      "source": [
        "**What You Need to Provide**:\n",
        "When calling model_training_pipeline_evaluation_and_prediction, you'll provide the following:\n",
        "- `is_two_step_model`: A boolean value that determines whether to use a two-step modeling approach.\n",
        "    - Accepted Values:\n",
        "        - True: The solution will first train a classification model to predict if a refund will occur (refund_flag), and then a regression model to predict the refund amount (refund_value or refund_proportion) for transactions identified as refunds.\n",
        "        - False: The solution will train a single regression model to directly predict the refund_value or refund_proportion.\n",
        "\n",
        "- `regression_model_type`: Specifies the type of regression model to use for predicting refund values.\n",
        "    - Accepted Values: Values from constant.LinearBigQueryMLModelType (e.g., LINEAR_REGRESSION), constant.DNNBigQueryMLModelType (e.g., DNN_REGRESSOR), or constant.BoostedTreeBigQueryMLModelType (e.g., BOOSTED_TREE_REGRESSOR).\n",
        "    - Use the appropriate constant, e.g., constant.LinearBigQueryMLModelType.LINEAR_REGRESSION.\n",
        "\n",
        "- `binary_classifier_model_type`: Specifies the type of binary classification model to use for predicting refund flags (if is_two_step_model is True).\n",
        "    - Accepted Values: Values from constant.LinearBigQueryMLModelType (e.g., LOGISTIC_REGRESSION), constant.DNNBigQueryMLModelType (e.g., DNN_CLASSIFIER), or constant.BoostedTreeBigQueryMLModelType (e.g., BOOSTED_TREE_CLASSIFIER).\n",
        "    - Use the appropriate constant, e.g., constant.LinearBigQueryMLModelType.LOGISTIC_REGRESSION.\n",
        "\n",
        "- `first_time_purchase`: A boolean flag that, when `use_ga4_data_for_feature_engineering` is True, tells the pipeline whether to train the model specifically for first-time purchasers or existing customers. This allows for tailored models based on customer behavior. This parameter is ignored if you're not using GA4 data.\n",
        "    - Accepted Values: True (for first-time purchase data) or False (for existing customer data).\n",
        "    - How to Provide: Set to True or False if `self.use_ga4_data_for_feature_engineering` is True.\n",
        "\n",
        "- `num_tiers_to_create_avg_prediction`: The number of tiers (or bins) to divide the predictions into for the \"tier-level average prediction vs. actual\" comparison. More tiers provide a more granular view. How to Provide: An integer value (e.g., 10).\n",
        "\n",
        "- `probability_threshold_for_prediction`: For binary classification models, this is the probability cutoff used to classify a transaction as a \"refund\" (1) or \"no refund\" (0) during prediction. For example, if set to 0.5, any predicted probability greater than or equal to 0.5 will be classified as a refund. How to Provide: A float value between 0 and 1 (e.g., 0.5).\n",
        "\n",
        "- `probability_threshold_for_model_evaluation`: Similar to probability_threshold_for_prediction, but specifically used for evaluating the binary classification model's performance metrics (e.g., calculating precision, recall, accuracy). You might use a different threshold for evaluation than for live prediction. How to Provide: A float value between 0 and 1 (e.g., 0.5).\n",
        "\n",
        "- `bqml_template_files_dir`: A mapping (dictionary) that provides the file paths to the BigQuery ML SQL query templates. These templates define the BigQuery ML operations for model training and prediction. This parameter usually uses a default value provided by the solution (constant.BQML_QUERY_TEMPLATE_FILES).\n",
        "   - **Note**: Typically, you won't need to change this and can use the default constant.\n",
        "- `**plot_kwargs`: This allows you to pass additional keyword arguments directly to the underlying `matplotlib.pyplot` functions used for generating plots (e.g., `figsize=(10, 6)` to control plot size, `dpi=300` for higher resolution).\n",
        "    - How to Provide: You can add arguments like `figsize=(10, 7)` directly to the function call.\n",
        "\n",
        "By successfully running this step, you'll have trained models, assessed their performance, and gained insights into their predictions and the factors influencing them. This is a crucial step towards understanding and leveraging your product return prediction solution!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6829d1a5-311b-4cf5-a946-88f82dd409c8",
      "metadata": {
        "id": "6829d1a5-311b-4cf5-a946-88f82dd409c8"
      },
      "outputs": [],
      "source": [
        "regression_model_type=constant.LinearBigQueryMLModelType.LINEAR_REGRESSION\n",
        "binary_classifier_model_type=constant.LinearBigQueryMLModelType.LOGISTIC_REGRESSION\n",
        "first_time_purchase=True\n",
        "performance_metrics_dfs, model_prediction_df, predictions_actuals_distribution, feature_importance_dfs = product_return.model_training_pipeline_evaluation_and_prediction(\n",
        "    is_two_step_model=True,\n",
        "    regression_model_type=regression_model_type,\n",
        "    binary_classifier_model_type=binary_classifier_model_type,\n",
        "    first_time_purchase=first_time_purchase,\n",
        "    num_tiers_to_create_avg_prediction=10,\n",
        "    probability_threshold_for_prediction=0.5,\n",
        "    probability_threshold_for_model_evaluation=0.5,\n",
        "    bqml_template_files_dir=constant.BQML_QUERY_TEMPLATE_FILES )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "856fe4ba-26a8-4025-a9c2-88b56ae0daab",
      "metadata": {
        "id": "856fe4ba-26a8-4025-a9c2-88b56ae0daab"
      },
      "source": [
        "The `model_training_pipeline_evaluation_and_prediction` method returns a tuple containing four key components. Each component is designed to give you a different perspective on your model's effectiveness.\n",
        " - `performance_metrics_dfs`: This output is a mapping (dictionary) where each key represents a type of model (e.g., 'regression_model', 'classification_model', 'two_step_model') and its corresponding value is a Pandas DataFrame. These DataFrames contain the calculated performance metrics for each model trained during the pipeline. These DataFrames are crucial for understanding how well your models are performing against the specified target variables.\n",
        "\n",
        " - For Regression Models: You'll typically find metrics like:\n",
        "     - Mean Absolute Error (MAE): The average of the absolute differences between predicted and actual values. It gives you a sense of the average magnitude of errors.\n",
        "     - Mean Squared Error (MSE): The average of the squared differences between predicted and actual values. It penalizes larger errors more heavily.\n",
        "     - Root Mean Squared Error (RMSE): The square root of MSE, often preferred as it's in the same units as the target variable.\n",
        "     - R-squared (R2): Represents the proportion of the variance in the dependent variable that's predictable from the independent variables. A higher R2 indicates a better fit.\n",
        "\n",
        "- For Classification Models: You'll typically find metrics like:\n",
        "    - Accuracy: The proportion of correctly classified instances (both true positives and true negatives) out of the total instances.\n",
        "    - Precision: Of all the instances predicted as positive, what proportion were actually positive. Useful when the cost of false positives is high.\n",
        "    - Recall (Sensitivity): Of all the actual positive instances, what proportion were correctly identified. Useful when the cost of false negatives is high.\n",
        "    - F1-Score: The harmonic mean of precision and recall. It's a good measure when you need a balance between precision and recall.\n",
        "    - AUC (Area Under the Receiver Operating Characteristic Curve): Measures the ability of a binary classifier to discriminate between positive and negative classes. A higher AUC indicates better discriminatory power.\n",
        "\n",
        "You can access specific model metrics like this:\n",
        "\n",
        "```\n",
        "# For regression model metrics\n",
        "print(performance_metrics_dfs['linear_regression_model'])\n",
        "\n",
        "#For classification model metrics (if is_two_step_model is True)\n",
        "print(performance_metrics_dfs['logistic_regression_model'])\n",
        "```\n",
        "\n",
        "- `model_prediction_df`: This is a Pandas DataFrame that contains the predictions generated by your trained model(s) on the input dataset (specifically, the preprocessed data that was used for training and testing). It includes the original transaction_id_col and transaction_date_col (if available), the actual refund value/flag, and the model's corresponding predictions. This DataFrame allows you to inspect individual predictions and compare them directly with the actual outcomes. It's the raw data behind the distribution plots and tier-level comparisons. It also serves as the basis for understanding model behavior at a granular level.\n",
        "    - Columns typically included:\n",
        "        - `transaction_id_col` (e.g., 'transaction_id')\n",
        "        - `transaction_date_col` (e.g., 'transaction_date')\n",
        "        - Actual target variable column(s) (e.g., 'refund_value', 'refund_flag')\n",
        "        - prediction (the model's predicted value)\n",
        "\n",
        "\n",
        "- `predictions_actuals_distribution`: This is a mapping (dictionary) where the keys are 'prediction' and, if use_prediction_pipeline was False (i.e., you were in a training pipeline), also 'actual'. The values are Pandas Series containing descriptive statistics (e.g., count, mean, standard deviation, min, max, quartiles) for the distribution of your model's predictions and the actual target values.  This provides a high-level summary of the statistical properties of your predictions and actuals. It's particularly useful for quickly checking for biases or unexpected ranges in the model's output compared to the real data. This output is directly related to the histograms plotted by the `plot_predictions_actuals_distribution` function.\n",
        "\n",
        "- `feature_importance_dfs`: This is a mapping (dictionary) where keys are the model types (e.g., 'linear_regression', 'logistic_regression') and values are Pandas DataFrames. Each DataFrame lists the features used by that specific model and their corresponding importance scores (or attribution values). What it tells you: Feature importance helps you understand which input variables contributed most significantly to the model's predictions.\n",
        "    - This is invaluable for:\n",
        "        - Interpretability: Gaining insights into the drivers of product returns.\n",
        "        - Feature Engineering: Identifying features that might need further refinement or new features that could be created.\n",
        "        - Domain Knowledge Validation: Confirming if the model's learned importances align with your business understanding.\n",
        "    - Columns typically included:\n",
        "        - feature (the name of the input column)\n",
        "        - attribution (the importance score for that feature)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa63544-1d49-4f8e-902f-8881483914a3",
      "metadata": {
        "id": "dfa63544-1d49-4f8e-902f-8881483914a3"
      },
      "source": [
        "Check out the model performance metrics for each of the model types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cce513bf-1000-488a-9d42-7d82d582d93b",
      "metadata": {
        "id": "cce513bf-1000-488a-9d42-7d82d582d93b"
      },
      "outputs": [],
      "source": [
        "for model_type in performance_metrics_dfs.keys():\n",
        "    print(f'performance_metrics for {model_type}:')\n",
        "    display(performance_metrics_dfs[model_type])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ca6347-f501-46c1-9bb7-52c3b5cf9cd5",
      "metadata": {
        "id": "d8ca6347-f501-46c1-9bb7-52c3b5cf9cd5",
        "tags": []
      },
      "source": [
        "Check out the feature importance for each of the model types:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6ab9fb7-4bf0-460a-94fc-c8ef55fc99e8",
      "metadata": {
        "id": "c6ab9fb7-4bf0-460a-94fc-c8ef55fc99e8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "for model_type in feature_importance_dfs.keys():\n",
        "    print(f'feature importance for {model_type}:')\n",
        "    display(feature_importance_dfs[model_type].sort_values(by='attribution', ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b0bac8a-817e-4d96-88a3-4ba52ac7877e",
      "metadata": {
        "id": "9b0bac8a-817e-4d96-88a3-4ba52ac7877e"
      },
      "source": [
        "You can check the model predictions dataframe on the training and test dataset here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67df9dba-76b9-4562-864d-bb72fe4f53a4",
      "metadata": {
        "id": "67df9dba-76b9-4562-864d-bb72fe4f53a4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_prediction_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4803cd37-c3b5-4a8f-a57f-b31d75501f9a",
      "metadata": {
        "id": "4803cd37-c3b5-4a8f-a57f-b31d75501f9a"
      },
      "source": [
        "You can compare the distribution of the predictions versus the actuals here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a89916-2b87-4020-a45a-8978b41816c0",
      "metadata": {
        "id": "85a89916-2b87-4020-a45a-8978b41816c0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "predictions_actuals_distribution['prediction']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0f2c39-76b8-4f4e-a9af-630ec800f99f",
      "metadata": {
        "id": "df0f2c39-76b8-4f4e-a9af-630ec800f99f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "predictions_actuals_distribution['actual']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e48e47-8cc1-4f23-af8d-fb83a86f8b25",
      "metadata": {
        "id": "99e48e47-8cc1-4f23-af8d-fb83a86f8b25"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/product_return_predictor/model_training_demo_notebook.ipynb?workspaceId=abeyuka:add_notebook::citc",
          "timestamp": 1753718592072
        },
        {
          "file_id": "/piper/depot/google3/third_party/professional_services/solutions/product_return_predictor/model_training_demo_notebook.ipynb?workspaceId=abeyuka:add_notebook::citc",
          "timestamp": 1753509118305
        }
      ]
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-cpu.2-11.m124",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m124"
    },
    "kernelspec": {
      "display_name": "Python 3 (Local)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
